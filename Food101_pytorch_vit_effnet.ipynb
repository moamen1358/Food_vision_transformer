{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "owJCnnQg_-T9",
    "outputId": "f1c334bf-8e91-40b9-9a13-5ac6f2c46c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'helper_pytorch'...\n",
      "remote: Enumerating objects: 15, done.\u001b[K\n",
      "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 15 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (15/15), 12.31 KiB | 206.00 KiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/moamen1358/helper_pytorch.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CGj5tZw3ENjb"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "from helper_pytorch.helper_functions import download_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbYUDp2NJNTL",
    "outputId": "34760978-ea19-43b7-e67a-7a50376f4246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITLNBOC4ICD7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_effnetb2_model(num_classes:int=3,\n",
    "                          seed:int=42):\n",
    "\n",
    "    # Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5SPlQHNKlDL",
    "outputId": "7e24e365-ee30-460f-c2ed-ab638e8cec4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35.2M/35.2M [00:01<00:00, 32.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "effnet_model, effnet_transform = create_effnetb2_model(num_classes=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yd8PI4XtIrzq",
    "outputId": "11b33ae0-9de7-40bf-f462-8cb7ed0de918"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 101]             --                   Partial\n",
       "‚îú‚îÄSequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
       "‚îÇ    ‚îî‚îÄConv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
       "‚îÇ    ‚îî‚îÄSequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
       "‚îÇ    ‚îî‚îÄSequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
       "‚îÇ    ‚îî‚îÄSequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
       "‚îÇ    ‚îî‚îÄSequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
       "‚îÇ    ‚îî‚îÄSequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
       "‚îÇ    ‚îî‚îÄSequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "‚îÇ    ‚îî‚îÄSequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
       "‚îÇ    ‚îî‚îÄConv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
       "‚îú‚îÄAdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
       "‚îú‚îÄSequential (classifier)                                    [1, 1408]            [1, 101]             --                   True\n",
       "‚îÇ    ‚îî‚îÄDropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
       "‚îÇ    ‚îî‚îÄLinear (1)                                            [1, 1408]            [1, 101]             142,309              True\n",
       "============================================================================================================================================\n",
       "Total params: 7,843,303\n",
       "Trainable params: 142,309\n",
       "Non-trainable params: 7,700,994\n",
       "Total mult-adds (Units.MEGABYTES): 657.78\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 156.80\n",
       "Params size (MB): 31.37\n",
       "Estimated Total Size (MB): 188.77\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Get a summary of EffNetB2 feature extractor for Food101 with 101 output classes (uncomment for full output)\n",
    "summary(effnet_model,\n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMEmEFCELF0_",
    "outputId": "6180d648-329b-4e83-90a3-56ea6d6da985"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
       "    ImageClassification(\n",
       "    crop_size=[288]\n",
       "    resize_size=[288]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_with_aug = torchvision.transforms.Compose([torchvision.transforms.TrivialAugmentWide(),\n",
    "                                                     effnet_transform])\n",
    "transform_with_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qwR-QRaFz2V",
    "outputId": "d2204bd0-71a7-4f93-f4da-a2dad791dd62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.00G/5.00G [03:41<00:00, 22.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "data_dir = Path(\"data\")\n",
    "train_data = torchvision.datasets.Food101(data_dir,\n",
    "                                          split=\"train\",\n",
    "                                          download=True,\n",
    "                                          transform=transform_with_aug)\n",
    "test_data = torchvision.datasets.Food101(data_dir,\n",
    "                                         split='test',\n",
    "                                         download=True,\n",
    "                                         transform=effnet_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4AG4wDWOxy_"
   },
   "outputs": [],
   "source": [
    "from helper_pytorch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rKAc7mIEPN3I"
   },
   "outputs": [],
   "source": [
    "train_dir = data_dir / \"train\"\n",
    "test_dir = data_dir / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EH8PuzMsPz7d"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Get class names\n",
    "class_names = train_data.classes\n",
    "\n",
    "# Turn images into data loaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkbkJFbtP8dT"
   },
   "outputs": [],
   "source": [
    "optimzer = torch.optim.Adam(params=effnet_model.parameters(),\n",
    "                             lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sfgca2RNQ32U",
    "outputId": "1f6ba27b-53f6-4f7f-b098-1545dc538720"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223,
     "referenced_widgets": [
      "ed4929aa4dd24a4e8bf60193cddc64c8",
      "7e565207095e48a4a8cdcd6cc9467903",
      "5cf2d3a04c6c49e08ca578a591b6b36c",
      "90238c183d2f436280e88793154d15b1",
      "068945bbb57149e68fabbc6109c7dc47",
      "34432e18b716481aaa1724b74a23d4e0",
      "a16f8f26ecea434b9eaeb7925827f90b",
      "deb61e6a8f004591b7a5f32be0dd2a0b",
      "6134491f47d64a328e7d0edbeecaf0a8",
      "b6214ea24ae4467eb805c3221e644ea4",
      "b91ccea5ca1c48baa08e8d98af93eb56"
     ]
    },
    "id": "jJrRroMsQJPI",
    "outputId": "fd6bdcbc-cba0-43fe-a125-a945ed480f2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4929aa4dd24a4e8bf60193cddc64c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 2.6228 | train_acc: 0.4050 | test_loss: 1.6172 | test_acc: 0.6047\n",
      "Epoch: 2 | train_loss: 2.1374 | train_acc: 0.4803 | test_loss: 1.4603 | test_acc: 0.6297\n",
      "Epoch: 3 | train_loss: 2.0753 | train_acc: 0.4951 | test_loss: 1.3895 | test_acc: 0.6426\n",
      "Epoch: 4 | train_loss: 2.0532 | train_acc: 0.4987 | test_loss: 1.3793 | test_acc: 0.6458\n",
      "Epoch: 5 | train_loss: 2.0344 | train_acc: 0.5033 | test_loss: 1.3807 | test_acc: 0.6413\n",
      "Epoch: 6 | train_loss: 2.0304 | train_acc: 0.5033 | test_loss: 1.3452 | test_acc: 0.6514\n",
      "Epoch: 7 | train_loss: 2.0246 | train_acc: 0.5047 | test_loss: 1.3488 | test_acc: 0.6487\n",
      "Epoch: 8 | train_loss: 2.0130 | train_acc: 0.5072 | test_loss: 1.3611 | test_acc: 0.6481\n",
      "Epoch: 9 | train_loss: 2.0096 | train_acc: 0.5099 | test_loss: 1.3536 | test_acc: 0.6470\n",
      "Epoch: 10 | train_loss: 2.0280 | train_acc: 0.5054 | test_loss: 1.3355 | test_acc: 0.6523\n"
     ]
    }
   ],
   "source": [
    "from helper_pytorch import engine\n",
    "effnet_result = engine.train(model=effnet_model,\n",
    "                            train_dataloader=train_dataloader,\n",
    "                            test_dataloader=test_dataloader,\n",
    "                            optimizer=optimzer,\n",
    "                            loss_fn=loss_fn,\n",
    "                            epochs=10,\n",
    "                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y9p1Eo8-fYRy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_vit_model(num_classes:int=3,\n",
    "                     seed:int=42):\n",
    "    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of target classes. Defaults to 3.\n",
    "        seed (int, optional): random seed value for output layer. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): ViT-B/16 feature extractor model.\n",
    "        transforms (torchvision.transforms): ViT-B/16 image transforms.\n",
    "    \"\"\"\n",
    "    # Create ViT_B_16 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "    # Freeze all layers in model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head to suit our needs (this will be trainable)\n",
    "    torch.manual_seed(seed)\n",
    "    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
    "                                          out_features=num_classes)) # update to reflect target number of classes\n",
    "\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EodJV9T3hAlZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exJELyMAfne2",
    "outputId": "e1f95289-ce26-487b-8368-bf073c437628"
   },
   "outputs": [],
   "source": [
    "vit_model, vit_transforms = create_vit_model(\n",
    "    num_classes=101, # could also use len(class_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H48_vta9hCaO",
    "outputId": "58b258ad-3a83-4f21-be7c-dd618e2f75a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 101]             768                  Partial\n",
       "‚îú‚îÄConv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False\n",
       "‚îú‚îÄEncoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n",
       "‚îÇ    ‚îî‚îÄDropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "‚îÇ    ‚îî‚îÄSequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "‚îÇ    ‚îî‚îÄLayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n",
       "‚îú‚îÄSequential (heads)                                         [1, 768]             [1, 101]             --                   True\n",
       "‚îÇ    ‚îî‚îÄLinear (0)                                            [1, 768]             [1, 101]             77,669               True\n",
       "============================================================================================================================================\n",
       "Total params: 85,876,325\n",
       "Trainable params: 77,669\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (M): 172.54\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 229.50\n",
       "Estimated Total Size (MB): 334.19\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Get a summary of EffNetB2 feature extractor for Food101 with 101 output classes (uncomment for full output)\n",
    "summary(vit_model,\n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDnbOy16hKbN",
    "outputId": "7000e71b-2a9d-4f20-e318-ce0ff7d6a7ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
       "    ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_with_aug = torchvision.transforms.Compose([torchvision.transforms.TrivialAugmentWide(),\n",
    "                                                     vit_transforms])\n",
    "transform_with_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove torchvision partial files (if present)\n",
    "!rm -f data/food-101.tar.gz\n",
    "!rm -rf data/food-101  # only if you want to force a fresh extract\n",
    "# then re-run the manual download commands above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGaUXc2WhV4G",
    "outputId": "c28ecad2-0ef0-4174-c27a-debd7bf5bad0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File not found or corrupted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      3\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFood101\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_with_aug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m test_data \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mFood101(data_dir,\n\u001b[1;32m      9\u001b[0m                                          split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                          download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m                                          transform\u001b[38;5;241m=\u001b[39mvit_transforms)\n",
      "File \u001b[0;32m~/miniconda3/envs/vit_env/lib/python3.10/site-packages/torchvision/datasets/food101.py:54\u001b[0m, in \u001b[0;36mFood101.__init__\u001b[0;34m(self, root, split, transform, target_transform, download, loader)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_images_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/vit_env/lib/python3.10/site-packages/torchvision/datasets/food101.py:98\u001b[0m, in \u001b[0;36mFood101._download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_MD5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vit_env/lib/python3.10/site-packages/torchvision/datasets/utils.py:388\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    386\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 388\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    391\u001b[0m extract_archive(archive, extract_root, remove_finished)\n",
      "File \u001b[0;32m~/miniconda3/envs/vit_env/lib/python3.10/site-packages/torchvision/datasets/utils.py:137\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# check integrity of downloaded file\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(fpath, md5):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found or corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File not found or corrupted."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "data_dir = Path(\"data\")\n",
    "train_data = torchvision.datasets.Food101(data_dir,\n",
    "                                          split=\"train\",\n",
    "                                          download=False,\n",
    "                                          transform=transform_with_aug)\n",
    "test_data = torchvision.datasets.Food101(data_dir,\n",
    "                                         split='test',\n",
    "                                         download=False,\n",
    "                                         transform=vit_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2VKBBUzhZas"
   },
   "outputs": [],
   "source": [
    "train_dir = data_dir / \"train\"\n",
    "test_dir = data_dir / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1kQrv4vhZNQ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Get class names\n",
    "class_names = train_data.classes\n",
    "\n",
    "# Turn images into data loaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aobJ8j7OgqHp"
   },
   "outputs": [],
   "source": [
    "optimzer = torch.optim.Adam(params=vit_model.parameters(),\n",
    "                             lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "f6208cf60bea467598708e7be17e2c94",
      "b6209c42d04446569004d0074454b9d8",
      "3937a17914174f3e8ca63ab571ec816e",
      "39b9424a0ea044639d2f6560cce2b9a3",
      "e76540f982d6412ba7ff5751a8b15e4a",
      "9abdb745ed304092bb8b354a6442735a",
      "44326644b6814a5bb279349a3a5efe45",
      "f5e49d3984dd442588eee5294d433011",
      "4394e5b1b59c4883b7414a33614ea848",
      "4449ca41795a4866bac07b36f3e73522",
      "ee49e54ffb694006875aa518ea7828e0"
     ]
    },
    "id": "rCLWCmqVfcqU",
    "outputId": "3564ced7-b193-47de-cdf5-576187c0d857"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6208cf60bea467598708e7be17e2c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from helper_pytorch import engine\n",
    "vit_result = engine.train(model=vit_model,\n",
    "                            train_dataloader=train_dataloader,\n",
    "                            test_dataloader=test_dataloader,\n",
    "                            optimizer=optimzer,\n",
    "                            loss_fn=loss_fn,\n",
    "                            epochs=3,\n",
    "                            device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwzG2SjPQCdG"
   },
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "# Check out the loss curves for FoodVision Big\n",
    "plot_loss_curves(vit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5l7iuSGP6w5"
   },
   "outputs": [],
   "source": [
    "from going_modular.going_modular import utils\n",
    "\n",
    "# Create a model path\n",
    "vit_food101_model_path = \"vit_model.pth\"\n",
    "\n",
    "# Save FoodVision Big model\n",
    "utils.save_model(model=vit_model,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=vit_food101_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1S5neEMNP9HC"
   },
   "outputs": [],
   "source": [
    "# Create Food101 compatible EffNetB2 instance\n",
    "loaded_vit_food101, vit_transforms = create_vit_model(num_classes=101)\n",
    "\n",
    "# Load the saved model's state_dict()\n",
    "loaded_vit_food101.load_state_dict(torch.load(\"models/vit_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djdss4t0P_Su"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_effnetb2_food101_model_size = Path(\"models\", vit_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
    "print(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvX0aoXISZj1"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "food_visin_path = Path(\"demos/foodvison_big/\")\n",
    "food_visin_path.mkdir(parents=True,\n",
    "                      exist_ok=True)\n",
    "(food_visin_path/ \"examples\").mkdir(parents=True,\n",
    "                                      exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWI0F4scMGho"
   },
   "outputs": [],
   "source": [
    "# Create path to Food101 class names\n",
    "foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n",
    "\n",
    "# Write Food101 class names list to file\n",
    "with open(foodvision_big_class_names_path, \"w\") as f:\n",
    "    print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n",
    "    f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okc0gw5lPq5M"
   },
   "outputs": [],
   "source": [
    "# Open Food101 class names file and read each line into a list\n",
    "with open(foodvision_big_class_names_path, \"r\") as f:\n",
    "    food101_class_names_loaded = [food.strip() for food in  f.readlines()]\n",
    "\n",
    "# View the first 5 class names loaded back in\n",
    "food101_class_names_loaded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfbkwOBpPsyl"
   },
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_vit_model(num_classes:int=3,\n",
    "                     seed:int=42):\n",
    "    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of target classes. Defaults to 3.\n",
    "        seed (int, optional): random seed value for output layer. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): ViT-B/16 feature extractor model.\n",
    "        transforms (torchvision.transforms): ViT-B/16 image transforms.\n",
    "    \"\"\"\n",
    "    # Create ViT_B_16 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "    # Freeze all layers in model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head to suit our needs (this will be trainable)\n",
    "    torch.manual_seed(seed)\n",
    "    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
    "                                          out_features=num_classes)) # update to reflect target number of classes\n",
    "\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMWC8bD8PvIN"
   },
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/app.py\n",
    "### 1. Imports and class names setup ###\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_vit_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Setup class names\n",
    "with open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt\n",
    "    class_names = [food_name.strip() for food_name in  f.readlines()]\n",
    "\n",
    "### 2. Model and transforms preparation ###\n",
    "\n",
    "# Create model\n",
    "effnetb2, effnetb2_transforms = create_vit_model(\n",
    "    num_classes=101, # could also use len(class_names)\n",
    ")\n",
    "\n",
    "# Load saved weights\n",
    "effnetb2.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",\n",
    "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
    "    )\n",
    ")\n",
    "\n",
    "### 3. Predict function ###\n",
    "\n",
    "# Create predict function\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "\n",
    "    # Transform the target image and add a batch dimension\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "\n",
    "    # Put model into evaluation mode and turn on inference mode\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "\n",
    "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "\n",
    "    # Calculate the prediction time\n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "\n",
    "    # Return the prediction dictionary and prediction time\n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "### 4. Gradio app ###\n",
    "\n",
    "# Create title, description and article strings\n",
    "title = \"FoodVision Big üçîüëÅ\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\"\n",
    "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "# Create examples list from \"examples/\" directory\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=[\n",
    "        gr.Label(num_top_classes=5, label=\"Predictions\"),\n",
    "        gr.Number(label=\"Prediction time (s)\"),\n",
    "    ],\n",
    "    examples=example_list,\n",
    "    title=title,\n",
    "    description=description,\n",
    "    article=article,\n",
    ")\n",
    "\n",
    "# Launch the app!\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbAbEeeaPxMI"
   },
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_big/requirements.txt\n",
    "torch==1.12.0\n",
    "torchvision==0.13.0\n",
    "gradio==3.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuxigGZVPzDV"
   },
   "outputs": [],
   "source": [
    "# Zip foodvision_big folder but exclude certain files\n",
    "!cd demos/foodvision_big && zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
    "\n",
    "# Download the zipped FoodVision Big app (if running in Google Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"demos/foodvision_big.zip\")\n",
    "except:\n",
    "    print(\"Not running in Google Colab, can't use google.colab.files.download()\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "068945bbb57149e68fabbc6109c7dc47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34432e18b716481aaa1724b74a23d4e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3937a17914174f3e8ca63ab571ec816e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5e49d3984dd442588eee5294d433011",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4394e5b1b59c4883b7414a33614ea848",
      "value": 0
     }
    },
    "39b9424a0ea044639d2f6560cce2b9a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4449ca41795a4866bac07b36f3e73522",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ee49e54ffb694006875aa518ea7828e0",
      "value": "‚Äá0/3‚Äá[00:00&lt;?,‚Äá?it/s]"
     }
    },
    "4394e5b1b59c4883b7414a33614ea848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44326644b6814a5bb279349a3a5efe45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4449ca41795a4866bac07b36f3e73522": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cf2d3a04c6c49e08ca578a591b6b36c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_deb61e6a8f004591b7a5f32be0dd2a0b",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6134491f47d64a328e7d0edbeecaf0a8",
      "value": 10
     }
    },
    "6134491f47d64a328e7d0edbeecaf0a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7e565207095e48a4a8cdcd6cc9467903": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34432e18b716481aaa1724b74a23d4e0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a16f8f26ecea434b9eaeb7925827f90b",
      "value": "100%"
     }
    },
    "90238c183d2f436280e88793154d15b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6214ea24ae4467eb805c3221e644ea4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b91ccea5ca1c48baa08e8d98af93eb56",
      "value": "‚Äá10/10‚Äá[3:22:56&lt;00:00,‚Äá1203.02s/it]"
     }
    },
    "9abdb745ed304092bb8b354a6442735a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a16f8f26ecea434b9eaeb7925827f90b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6209c42d04446569004d0074454b9d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9abdb745ed304092bb8b354a6442735a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_44326644b6814a5bb279349a3a5efe45",
      "value": "‚Äá‚Äá0%"
     }
    },
    "b6214ea24ae4467eb805c3221e644ea4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b91ccea5ca1c48baa08e8d98af93eb56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "deb61e6a8f004591b7a5f32be0dd2a0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e76540f982d6412ba7ff5751a8b15e4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed4929aa4dd24a4e8bf60193cddc64c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e565207095e48a4a8cdcd6cc9467903",
       "IPY_MODEL_5cf2d3a04c6c49e08ca578a591b6b36c",
       "IPY_MODEL_90238c183d2f436280e88793154d15b1"
      ],
      "layout": "IPY_MODEL_068945bbb57149e68fabbc6109c7dc47"
     }
    },
    "ee49e54ffb694006875aa518ea7828e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5e49d3984dd442588eee5294d433011": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6208cf60bea467598708e7be17e2c94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b6209c42d04446569004d0074454b9d8",
       "IPY_MODEL_3937a17914174f3e8ca63ab571ec816e",
       "IPY_MODEL_39b9424a0ea044639d2f6560cce2b9a3"
      ],
      "layout": "IPY_MODEL_e76540f982d6412ba7ff5751a8b15e4a"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
